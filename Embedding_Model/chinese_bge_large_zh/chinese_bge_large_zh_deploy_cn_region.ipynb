{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724accfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade boto3 -i https://pypi.tuna.tsinghua.edu.cn/simple/\n",
    "!pip install --upgrade sagemaker==2.183.0 -i https://pypi.tuna.tsinghua.edu.cn/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc53fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For notebook instances (Amazon Linux)\n",
    "!sudo yum update -y\n",
    "!sudo yum install amazon-linux-extras\n",
    "!sudo amazon-linux-extras install epel -y\n",
    "!sudo yum update -y\n",
    "!sudo yum install git-lfs git -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05457791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c4758",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Helper: Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # Load model from HuggingFace Hub\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModel.from_pretrained(model_dir)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    # destruct model and tokenizer\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = data.pop(\"inputs\", data)\n",
    "#     encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    instruction = \"为这个句子生成表示以用于检索相关文章：\"\n",
    "    encoded_input = tokenizer([instruction + q for q in sentences], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        # Perform pooling. In this case, cls pooling.\n",
    "        sentence_embeddings = model_output[0][:, 0]\n",
    "    \n",
    "    sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    # return dictonary, which will be json serializable\n",
    "    return sentence_embeddings.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository = \"BAAI/bge-large-zh-v1.5\"\n",
    "model_id=repository.split(\"/\")[-1]\n",
    "s3_location=f\"s3://{sess.default_bucket()}/custom_inference/{model_id}/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c90b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs install\n",
    "#!git clone https://huggingface.co/$repository\n",
    "!git clone https://www.modelscope.cn/Xorbits/bge-large-zh-v1.5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cdf055",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r code/ $model_id/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17749164",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $model_id\n",
    "!tar zcvf model.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a8959",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp model.tar.gz $s3_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'BAAI/bge-large-zh-v1.5',\n",
    "\t'HF_TASK':'feature-extraction'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_location,       # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.12\",  # transformers version used\n",
    "   pytorch_version=\"1.9\",        # pytorch version used\n",
    "   py_version='py38',            # python version used\n",
    "   #env=hub,\n",
    ")\n",
    "\n",
    "endpoint_name = 'huggingface-inference-eb'\n",
    "# instance_type = 'ml.g4dn.2xlarge'\n",
    "instance_type = 'ml.g4dn.xlarge'\n",
    "instance_count = 1\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "predictor = huggingface_model.deploy(\n",
    "    endpoint_name = endpoint_name,\n",
    "    instance_type = instance_type, \n",
    "    initial_instance_count = instance_count,\n",
    "    serializer = JSONSerializer(),\n",
    "    deserializer = JSONDeserializer()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference testing\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "hfp = sagemaker.huggingface.model.HuggingFacePredictor('huggingface-inference-eb')\n",
    "\n",
    "embedding = hfp.predict({'inputs':['测试一下']})\n",
    "print(embedding)\n",
    "print(len(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab3b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
